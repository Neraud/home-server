# See https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: workflow-controller-configmap
data:
  # Parallelism limits the max total parallel workflows that can execute at the same time
  # (available since Argo v2.3). Controller must be restarted to take effect.
  parallelism: "10"

  # Limit the maximum number of incomplete workflows in a namespace.
  # Intended for cluster installs that are multi-tenancy environments, to prevent too many workflows in one
  # namespace impacting others.
  # >= v3.2
  namespaceParallelism: "10"

  # Globally limits the rate at which pods are created.
  # This is intended to mitigate flooding of the Kubernetes API server by workflows with a large amount of
  # parallel nodes.
  resourceRateLimit: |
    limit: 10
    burst: 1

  # Whether or not to emit events on node completion. These can take a up a lot of space in
  # k8s (typically etcd) resulting in errors when trying to create new events:
  # "Unable to create audit event: etcdserver: mvcc: database space exceeded"
  # This config item allows you to disable this.
  # (since v2.9)
  nodeEvents: |
    enabled: true

  # uncomment following lines if workflow controller runs in a different k8s cluster with the
  # workflow workloads, or needs to communicate with the k8s apiserver using an out-of-cluster
  # kubeconfig secret
  # kubeConfig:
  #   # name of the kubeconfig secret, may not be empty when kubeConfig specified
  #   secretName: kubeconfig-secret
  #   # key of the kubeconfig secret, may not be empty when kubeConfig specified
  #   secretKey: kubeconfig
  #   # mounting path of the kubeconfig secret, default to /kube/config
  #   mountPath: /kubeconfig/mount/path
  #   # volume name when mounting the secret, default to kubeconfig
  #   volumeName: kube-config-volume

  # artifactRepository defines the default location to be used as the artifact repository for
  # container artifacts.
  artifactRepository: |
    # archiveLogs will archive the main container logs as an artifact
    archiveLogs: true

    s3:
      # Use the corresponding endpoint depending on your S3 provider:
      #   AWS: s3.amazonaws.com
      #   GCS: storage.googleapis.com
      #   Minio: my-minio-endpoint.default:9000
      endpoint: minio.dev-minio.svc.cluster.local:9000
      bucket: argo-artifacts
      #region: us-west-2
      # insecure will disable TLS. Primarily used for minio installs not configured with TLS
      insecure: true
      # keyFormat is a format pattern to define how artifacts will be organized in a bucket.
      # It can reference workflow metadata variables such as workflow.namespace, workflow.name,
      # pod.name. Can also use strftime formating of workflow.creationTimestamp so that workflow
      # artifacts can be organized by date. If omitted, will use `{{workflow.name}}/{{pod.name}}`,
      # which has potential for have collisions.
      # The following example pattern organizes workflow artifacts under a "my-artifacts" sub dir,
      # then sub dirs for year, month, date and finally workflow name and pod.
      # e.g.: my-artifacts/2018/08/23/my-workflow-abc123/my-workflow-abc123-1234567890
      keyFormat: "artifacts\
        /{{workflow.creationTimestamp.Y}}\
        /{{workflow.creationTimestamp.m}}\
        /{{workflow.creationTimestamp.d}}\
        /{{workflow.name}}\
        /{{pod.name}}"
      # The actual secret object (in this example my-s3-credentials), should be created in every
      # namespace where a workflow needs to store its artifacts to S3. If omitted,
      # attempts to use IAM role to access the bucket (instead of accessKey/secretKey).
      accessKeySecret:
        name: minio-credentials
        key: accessKey
      secretKeySecret:
        name: minio-credentials
        key: secretKey
      # If this is set to true, argo workflows will use AWS SDK default credentials provider chain. This will allow things like
      # IRSA and any of the authentication methods that the golang SDK uses in it's default chain.
      # If you are using IRSA on AWS, and set this option to true, you will also need to modify Argo-Server Deployment with
      # `spec.template.spec.securityContext.fsGroup: 65534` configuration. This is required for IRSA to be able to access
      # `/var/run/secrets/eks.amazonaws.com/serviceaccount/token` file, and authenticate with AWS.
      useSDKCreds: false
      encryptionOptions:
        # If this is set to true, SSE-S3 encryption will be used to store objects
        # unless kmsKeyId or serverSideCustomerKeySecret is set
        enableEncryption: false
        # A valid kms key id. If this value is set, the object stored in s3 will be encrypted with SSE-KMS
        # Note: You cannot set both kmsKeyId and serverSideCustomerKeySecret
        # kmsKeyId: ''
        # Allows you to set a json blob of simple key value pairs. See
        # https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context
        # for more information
        # kmsEncryptionContext: ''
        # The actual secret object (in this example my-s3-credentials),
        # should be created when using a custom secret to encrypt objects in using SSE-C
        # Note: You cannot set both kmsKeyId and serverSideCustomerKeySecret
        # serverSideCustomerKeySecret:
        #  name: my-s3-credentials
        #  key: secretKey

  # Defaults for main containers. These can be overridden by the template.
  # <= v3.3 only `resources` are supported.
  # >= v3.4 all fields are supported, including security context.
  mainContainer: |
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 0.1
        memory: 64Mi
      limits:
        cpu: 0.5
        memory: 512Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 1000

  # executor controls how the init and wait container should be customized
  # (available since Argo v2.3)
  executor: |
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 0.1
        memory: 64Mi
      limits:
        cpu: 0.5
        memory: 512Mi
    # args & env allows command line arguments and environment variables to be appended to the
    # executor container and is mainly used for development/debugging purposes.
    args:
    - --loglevel
    - debug
    - --gloglevel
    - "6"
    env:
    # ARGO_TRACE enables some tracing information for debugging purposes. Currently it enables
    # logging of S3 request/response payloads (including auth headers)
    - name: ARGO_TRACE
      value: "1"

  # metricsConfig controls the path and port for prometheus metrics. Metrics are enabled and emitted on localhost:9090/metrics
  # by default.
  metricsConfig: |
    # Enabled controls metric emission. Default is true, set "enabled: false" to turn off
    enabled: true
    # Path is the path where metrics are emitted. Must start with a "/". Default is "/metrics"
    path: /metrics
    # Port is the port where metrics are emitted. Default is "9090"
    port: 9090
    # MetricsTTL sets how often custom metrics are cleared from memory. Default is "0", metrics are never cleared
    metricsTTL: "10m"
    # IgnoreErrors is a flag that instructs prometheus to ignore metric emission errors. Default is "false"
    ignoreErrors: false
    # Use a self-signed cert for TLS, default false
    secure: false

    # DEPRECATED: Legacy metrics are now removed, this field is ignored
    disableLegacy: false

  # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level
  # See more: docs/default-workflow-specs.md
  #workflowDefaults: |
  #  metadata:
  #    annotations:
  #      argo: workflows
  #    labels:
  #      foo: bar
  #  spec:
  #    ttlStrategy:
  #      secondsAfterSuccess: 5
  #    parallelism: 3

  # workflowRestrictions restricts the Workflows that the controller will process.
  # Current options:
  #   Strict: Only Workflows using "workflowTemplateRef" will be processed. This allows the administrator of the controller
  #     to set a "library" of templates that may be run by its operator, limiting arbitrary Workflow execution.
  #   Secure: Only Workflows using "workflowTemplateRef" will be processed and the controller will enforce
  #     that the WorkflowTemplate that is referenced hasn't changed between operations. If you want to make sure the operator of the
  #     Workflow cannot run an arbitrary Workflow, use this option.
  #workflowRestrictions: |
  #  templateReferencing: Strict
